Importing exception
Importing exception
Arguments:
{'additive_gauss_std': None,
 'anneal': False,
 'arch': 'hyperunet',
 'arr_dataset': False,
 'batch_size': 32,
 'beta': 128.0,
 'cont': 0,
 'data_path': '/share/sablab/nfs02/users/aw847/data/brain/abide/',
 'date': 'Sep_29',
 'epoch_of_p_max': None,
 'filename_prefix': 'div_prior_3terms',
 'forward_type': 'csmri',
 'gpu_id': 0,
 'hnet_hdim': 128,
 'hypernet_baseline_fit_layer_idx': None,
 'hyperparameters': None,
 'image_dims': (160, 224),
 'load': None,
 'log_interval': 25,
 'loss_list': ['l1', 'ssim'],
 'lr': 0.001,
 'mask_type': 'epi_vertical',
 'method': 'uniform_diversity_prior',
 'models_dir': '/share/sablab/nfs02/users/aw847/models/HyperRecon/',
 'n_ch_out': 1,
 'num_epochs': 1024,
 'num_steps_per_epoch': 256,
 'num_train_subjects': 50,
 'num_val_subjects': 1,
 'optimizer_type': 'adam',
 'p_max': None,
 'p_min': None,
 'range_restrict': True,
 'run_dir': '/share/sablab/nfs02/users/aw847/models/HyperRecon/div_prior_3terms/Sep_29/archhyperunet_methoduniform_diversity_prior_rate4_lr0.001_bs32_l1+ssim_hnet128_unet32_topKNone_restrictTrue_hpNone_beta128.0',
 'scheduler_gamma': 0.5,
 'scheduler_step_size': 128,
 'seed': 1,
 'topK': None,
 'undersampling_rate': '4',
 'unet_hdim': 32,
 'unet_residual': False,
 'use_batchnorm': True}
Gathering subjects for dataloader
done
Gathering subjects for dataloader
done
Gathering subjects for dataloader
done
Loading mask: /share/sablab/nfs02/users/aw847/data/masks/EPI_vertical_4_160_224.npy

Model Summary
---------------------------------------------------------------
hnet.lin1.weight
hnet.lin1.bias
hnet.lin2.weight
hnet.lin2.bias
hnet.lin3.weight
hnet.lin3.bias
hnet.lin4.weight
hnet.lin4.bias
unet.dconv_down1.0.hyperkernel.weight
unet.dconv_down1.0.hyperkernel.bias
unet.dconv_down1.0.hyperbias.weight
unet.dconv_down1.0.hyperbias.bias
unet.dconv_down1.1.weight
unet.dconv_down1.1.bias
unet.dconv_down1.3.hyperkernel.weight
unet.dconv_down1.3.hyperkernel.bias
unet.dconv_down1.3.hyperbias.weight
unet.dconv_down1.3.hyperbias.bias
unet.dconv_down1.4.weight
unet.dconv_down1.4.bias
unet.dconv_down2.0.hyperkernel.weight
unet.dconv_down2.0.hyperkernel.bias
unet.dconv_down2.0.hyperbias.weight
unet.dconv_down2.0.hyperbias.bias
unet.dconv_down2.1.weight
unet.dconv_down2.1.bias
unet.dconv_down2.3.hyperkernel.weight
unet.dconv_down2.3.hyperkernel.bias
unet.dconv_down2.3.hyperbias.weight
unet.dconv_down2.3.hyperbias.bias
unet.dconv_down2.4.weight
unet.dconv_down2.4.bias
unet.dconv_down3.0.hyperkernel.weight
unet.dconv_down3.0.hyperkernel.bias
unet.dconv_down3.0.hyperbias.weight
unet.dconv_down3.0.hyperbias.bias
unet.dconv_down3.1.weight
unet.dconv_down3.1.bias
unet.dconv_down3.3.hyperkernel.weight
unet.dconv_down3.3.hyperkernel.bias
unet.dconv_down3.3.hyperbias.weight
unet.dconv_down3.3.hyperbias.bias
unet.dconv_down3.4.weight
unet.dconv_down3.4.bias
unet.dconv_down4.0.hyperkernel.weight
unet.dconv_down4.0.hyperkernel.bias
unet.dconv_down4.0.hyperbias.weight
unet.dconv_down4.0.hyperbias.bias
unet.dconv_down4.1.weight
unet.dconv_down4.1.bias
unet.dconv_down4.3.hyperkernel.weight
unet.dconv_down4.3.hyperkernel.bias
unet.dconv_down4.3.hyperbias.weight
unet.dconv_down4.3.hyperbias.bias
unet.dconv_down4.4.weight
unet.dconv_down4.4.bias
unet.dconv_up3.0.hyperkernel.weight
unet.dconv_up3.0.hyperkernel.bias
unet.dconv_up3.0.hyperbias.weight
unet.dconv_up3.0.hyperbias.bias
unet.dconv_up3.1.weight
unet.dconv_up3.1.bias
unet.dconv_up3.3.hyperkernel.weight
unet.dconv_up3.3.hyperkernel.bias
unet.dconv_up3.3.hyperbias.weight
unet.dconv_up3.3.hyperbias.bias
unet.dconv_up3.4.weight
unet.dconv_up3.4.bias
unet.dconv_up2.0.hyperkernel.weight
unet.dconv_up2.0.hyperkernel.bias
unet.dconv_up2.0.hyperbias.weight
unet.dconv_up2.0.hyperbias.bias
unet.dconv_up2.1.weight
unet.dconv_up2.1.bias
unet.dconv_up2.3.hyperkernel.weight
unet.dconv_up2.3.hyperkernel.bias
unet.dconv_up2.3.hyperbias.weight
unet.dconv_up2.3.hyperbias.bias
unet.dconv_up2.4.weight
unet.dconv_up2.4.bias
unet.dconv_up1.0.hyperkernel.weight
unet.dconv_up1.0.hyperkernel.bias
unet.dconv_up1.0.hyperbias.weight
unet.dconv_up1.0.hyperbias.bias
unet.dconv_up1.1.weight
unet.dconv_up1.1.bias
unet.dconv_up1.3.hyperkernel.weight
unet.dconv_up1.3.hyperkernel.bias
unet.dconv_up1.3.hyperbias.weight
unet.dconv_up1.3.hyperbias.bias
unet.dconv_up1.4.weight
unet.dconv_up1.4.bias
unet.conv_last.hyperkernel.weight
unet.conv_last.hyperkernel.bias
unet.conv_last.hyperbias.weight
unet.conv_last.hyperbias.bias
---------------------------------------------------------------
Number of main weights: 148513
Total parameters: 19208993
---------------------------------------------------------------

Loading checkpoint from /share/sablab/nfs02/users/aw847/models/HyperRecon/div_prior_3terms/Sep_29/archhyperunet_methoduniform_diversity_prior_rate4_lr0.001_bs32_l1+ssim_hnet128_unet32_topKNone_restrictTrue_hpNone_beta128.0/checkpoints/model.0375.h5

Epoch 376/1024
Learning rate: [0.00025]
train loss=1.183550, train psnr=27.107018, train time=156.348728
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 377/1024
Learning rate: [0.00025]
train loss=1.186509, train psnr=27.091155, train time=156.174316
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 378/1024
Learning rate: [0.00025]
train loss=1.183343, train psnr=27.232608, train time=156.705912
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 379/1024
Learning rate: [0.00025]
train loss=1.181878, train psnr=27.168115, train time=156.185038
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 380/1024
Learning rate: [0.00025]
train loss=1.186386, train psnr=27.156638, train time=156.445974
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 381/1024
Learning rate: [0.00025]
train loss=1.186002, train psnr=27.161252, train time=156.434817
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 382/1024
Learning rate: [0.00025]
train loss=1.185768, train psnr=27.148042, train time=156.874768
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 383/1024
Learning rate: [0.00025]
train loss=1.187959, train psnr=27.054884, train time=156.726408
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 384/1024
Learning rate: [0.00025]
train loss=1.185377, train psnr=27.193142, train time=156.803993
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 385/1024
Learning rate: [0.000125]
train loss=1.178991, train psnr=27.253423, train time=157.187330
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 386/1024
Learning rate: [0.000125]
train loss=1.179957, train psnr=27.221727, train time=156.919292
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 387/1024
Learning rate: [0.000125]
train loss=1.178673, train psnr=27.308811, train time=156.863347
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 388/1024
Learning rate: [0.000125]
train loss=1.178936, train psnr=27.277423, train time=156.619136
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 389/1024
Learning rate: [0.000125]
train loss=1.183795, train psnr=27.191437, train time=156.520692
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 390/1024
Learning rate: [0.000125]
train loss=1.179249, train psnr=27.187296, train time=156.751900
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 391/1024
Learning rate: [0.000125]
train loss=1.180680, train psnr=27.274144, train time=156.670638
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 392/1024
Learning rate: [0.000125]
train loss=1.182793, train psnr=27.196276, train time=156.779376
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 393/1024
Learning rate: [0.000125]
train loss=1.179208, train psnr=27.337250, train time=156.515536
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 394/1024
Learning rate: [0.000125]
train loss=1.180505, train psnr=27.298346, train time=156.652783
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 395/1024
Learning rate: [0.000125]
train loss=1.180360, train psnr=27.225352, train time=156.857755
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 396/1024
Learning rate: [0.000125]
train loss=1.178827, train psnr=27.254903, train time=156.832821
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 397/1024
Learning rate: [0.000125]
train loss=1.180490, train psnr=27.166986, train time=156.902875
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 398/1024
Learning rate: [0.000125]
train loss=1.182641, train psnr=27.157829, train time=156.957752
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 399/1024
Learning rate: [0.000125]
train loss=1.181270, train psnr=27.293806, train time=158.100334
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 400/1024
Learning rate: [0.000125]
train loss=1.181779, train psnr=27.211773, train time=157.287046
Validating with hparam 0.0
Validating with hparam 1.0
Saved checkpoint to /share/sablab/nfs02/users/aw847/models/HyperRecon/div_prior_3terms/Sep_29/archhyperunet_methoduniform_diversity_prior_rate4_lr0.001_bs32_l1+ssim_hnet128_unet32_topKNone_restrictTrue_hpNone_beta128.0/checkpoints/model.0400.h5

Epoch 401/1024
Learning rate: [0.000125]
train loss=1.178880, train psnr=27.198790, train time=156.819384
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 402/1024
Learning rate: [0.000125]
train loss=1.179813, train psnr=27.236125, train time=156.304828
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 403/1024
Learning rate: [0.000125]
train loss=1.181677, train psnr=27.314352, train time=155.875643
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 404/1024
Learning rate: [0.000125]
train loss=1.179949, train psnr=27.242840, train time=156.037332
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 405/1024
Learning rate: [0.000125]
train loss=1.175819, train psnr=27.261122, train time=155.955809
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 406/1024
Learning rate: [0.000125]
train loss=1.180787, train psnr=27.220451, train time=156.067458
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 407/1024
Learning rate: [0.000125]
train loss=1.179456, train psnr=27.216657, train time=156.232553
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 408/1024
Learning rate: [0.000125]
train loss=1.180387, train psnr=27.242309, train time=155.919408
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 409/1024
Learning rate: [0.000125]
train loss=1.179311, train psnr=27.250768, train time=156.004427
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 410/1024
Learning rate: [0.000125]
train loss=1.181413, train psnr=27.106673, train time=155.756809
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 411/1024
Learning rate: [0.000125]
train loss=1.180907, train psnr=27.163517, train time=156.286021
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 412/1024
Learning rate: [0.000125]
train loss=1.176459, train psnr=27.304453, train time=155.755747
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 413/1024
Learning rate: [0.000125]
train loss=1.179503, train psnr=27.262266, train time=155.747390
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 414/1024
Learning rate: [0.000125]
train loss=1.180203, train psnr=27.302797, train time=155.821133
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 415/1024
Learning rate: [0.000125]
train loss=1.177932, train psnr=27.315136, train time=156.115339
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 416/1024
Learning rate: [0.000125]
train loss=1.181787, train psnr=27.108971, train time=155.888499
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 417/1024
Learning rate: [0.000125]
train loss=1.175808, train psnr=27.338728, train time=155.986535
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 418/1024
Learning rate: [0.000125]
train loss=1.177440, train psnr=27.337114, train time=156.187314
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 419/1024
Learning rate: [0.000125]
train loss=1.177637, train psnr=27.292674, train time=155.934866
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 420/1024
Learning rate: [0.000125]
train loss=1.181356, train psnr=27.186761, train time=155.993712
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 421/1024
Learning rate: [0.000125]
train loss=1.178947, train psnr=27.316163, train time=155.950558
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 422/1024
Learning rate: [0.000125]
train loss=1.181766, train psnr=27.119984, train time=155.947555
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 423/1024
Learning rate: [0.000125]
train loss=1.178450, train psnr=27.258396, train time=156.193941
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 424/1024
Learning rate: [0.000125]
train loss=1.180183, train psnr=27.318714, train time=155.719191
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 425/1024
Learning rate: [0.000125]
train loss=1.178220, train psnr=27.267612, train time=155.832363
Validating with hparam 0.0
Validating with hparam 1.0
Saved checkpoint to /share/sablab/nfs02/users/aw847/models/HyperRecon/div_prior_3terms/Sep_29/archhyperunet_methoduniform_diversity_prior_rate4_lr0.001_bs32_l1+ssim_hnet128_unet32_topKNone_restrictTrue_hpNone_beta128.0/checkpoints/model.0425.h5

Epoch 426/1024
Learning rate: [0.000125]
train loss=1.178939, train psnr=27.187986, train time=156.143929
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 427/1024
Learning rate: [0.000125]
train loss=1.180937, train psnr=27.129684, train time=156.072706
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 428/1024
Learning rate: [0.000125]
train loss=1.179161, train psnr=27.280229, train time=156.131958
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 429/1024
Learning rate: [0.000125]
train loss=1.181384, train psnr=27.245671, train time=155.906481
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 430/1024
Learning rate: [0.000125]
train loss=1.177415, train psnr=27.207607, train time=156.019814
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 431/1024
Learning rate: [0.000125]
train loss=1.180924, train psnr=27.280730, train time=156.118639
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 432/1024
Learning rate: [0.000125]
train loss=1.178993, train psnr=27.309481, train time=156.237468
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 433/1024
Learning rate: [0.000125]
train loss=1.179400, train psnr=27.293435, train time=155.815375
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 434/1024
Learning rate: [0.000125]
train loss=1.179417, train psnr=27.271552, train time=155.960221
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 435/1024
Learning rate: [0.000125]
train loss=1.181873, train psnr=27.080952, train time=156.028437
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 436/1024
Learning rate: [0.000125]
train loss=1.177477, train psnr=27.285591, train time=155.949700
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 437/1024
Learning rate: [0.000125]
train loss=1.177495, train psnr=27.219813, train time=156.423043
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 438/1024
Learning rate: [0.000125]
train loss=1.175917, train psnr=27.379201, train time=155.985901
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 439/1024
Learning rate: [0.000125]
train loss=1.178329, train psnr=27.327273, train time=156.308384
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 440/1024
Learning rate: [0.000125]
train loss=1.178775, train psnr=27.174285, train time=156.126223
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 441/1024
Learning rate: [0.000125]
train loss=1.179265, train psnr=27.309813, train time=156.296042
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 442/1024
Learning rate: [0.000125]
train loss=1.178451, train psnr=27.267067, train time=156.207107
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 443/1024
Learning rate: [0.000125]
train loss=1.177145, train psnr=27.281965, train time=156.169255
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 444/1024
Learning rate: [0.000125]
train loss=1.177614, train psnr=27.207421, train time=156.209586
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 445/1024
Learning rate: [0.000125]
train loss=1.178573, train psnr=27.250911, train time=155.301297
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 446/1024
Learning rate: [0.000125]
train loss=1.174755, train psnr=27.377199, train time=155.325437
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 447/1024
Learning rate: [0.000125]
train loss=1.177427, train psnr=27.287934, train time=155.000401
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 448/1024
Learning rate: [0.000125]
train loss=1.179935, train psnr=27.192122, train time=155.290098
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 449/1024
Learning rate: [0.000125]
train loss=1.176789, train psnr=27.290107, train time=155.216199
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 450/1024
Learning rate: [0.000125]
train loss=1.180247, train psnr=27.147379, train time=155.175425
Validating with hparam 0.0
Validating with hparam 1.0
Saved checkpoint to /share/sablab/nfs02/users/aw847/models/HyperRecon/div_prior_3terms/Sep_29/archhyperunet_methoduniform_diversity_prior_rate4_lr0.001_bs32_l1+ssim_hnet128_unet32_topKNone_restrictTrue_hpNone_beta128.0/checkpoints/model.0450.h5

Epoch 451/1024
Learning rate: [0.000125]
train loss=1.176708, train psnr=27.250327, train time=155.189218
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 452/1024
Learning rate: [0.000125]
train loss=1.177663, train psnr=27.287590, train time=155.120816
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 453/1024
Learning rate: [0.000125]
train loss=1.178426, train psnr=27.269203, train time=155.044220
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 454/1024
Learning rate: [0.000125]
train loss=1.178053, train psnr=27.270368, train time=155.120069
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 455/1024
Learning rate: [0.000125]
train loss=1.180269, train psnr=27.194841, train time=155.298032
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 456/1024
Learning rate: [0.000125]
train loss=1.177444, train psnr=27.287384, train time=155.060793
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 457/1024
Learning rate: [0.000125]
train loss=1.180150, train psnr=27.298768, train time=155.200401
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 458/1024
Learning rate: [0.000125]
train loss=1.178672, train psnr=27.293343, train time=155.090873
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 459/1024
Learning rate: [0.000125]
train loss=1.182588, train psnr=27.189356, train time=155.382893
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 460/1024
Learning rate: [0.000125]
train loss=1.179347, train psnr=27.247928, train time=155.356253
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 461/1024
Learning rate: [0.000125]
train loss=1.179725, train psnr=27.224667, train time=155.388964
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 462/1024
Learning rate: [0.000125]
train loss=1.178010, train psnr=27.220596, train time=155.365501
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 463/1024
Learning rate: [0.000125]
train loss=1.178269, train psnr=27.121687, train time=155.281715
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 464/1024
Learning rate: [0.000125]
train loss=1.179797, train psnr=27.197049, train time=155.068730
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 465/1024
Learning rate: [0.000125]
train loss=1.177882, train psnr=27.267212, train time=154.793946
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 466/1024
Learning rate: [0.000125]
train loss=1.176984, train psnr=27.250225, train time=155.590032
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 467/1024
Learning rate: [0.000125]
train loss=1.177917, train psnr=27.394665, train time=155.145410
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 468/1024
Learning rate: [0.000125]
train loss=1.176816, train psnr=27.277194, train time=155.533059
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 469/1024
Learning rate: [0.000125]
train loss=1.177430, train psnr=27.256815, train time=155.121323
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 470/1024
Learning rate: [0.000125]
train loss=1.178278, train psnr=27.255399, train time=155.203126
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 471/1024
Learning rate: [0.000125]
train loss=1.178778, train psnr=27.244829, train time=157.060650
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 472/1024
Learning rate: [0.000125]
train loss=1.181223, train psnr=27.197234, train time=162.996704
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 473/1024
Learning rate: [0.000125]
train loss=1.174640, train psnr=27.284805, train time=159.011791
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 474/1024
Learning rate: [0.000125]
train loss=1.177006, train psnr=27.203664, train time=155.931365
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 475/1024
Learning rate: [0.000125]
train loss=1.178101, train psnr=27.219726, train time=156.096727
Validating with hparam 0.0
Validating with hparam 1.0
Saved checkpoint to /share/sablab/nfs02/users/aw847/models/HyperRecon/div_prior_3terms/Sep_29/archhyperunet_methoduniform_diversity_prior_rate4_lr0.001_bs32_l1+ssim_hnet128_unet32_topKNone_restrictTrue_hpNone_beta128.0/checkpoints/model.0475.h5

Epoch 476/1024
Learning rate: [0.000125]
train loss=1.179375, train psnr=27.136478, train time=156.313727
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 477/1024
Learning rate: [0.000125]
train loss=1.177193, train psnr=27.266043, train time=155.784452
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 478/1024
Learning rate: [0.000125]
train loss=1.176654, train psnr=27.269644, train time=155.884217
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 479/1024
Learning rate: [0.000125]
train loss=1.177407, train psnr=27.195059, train time=155.971324
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 480/1024
Learning rate: [0.000125]
train loss=1.177252, train psnr=27.244364, train time=156.390629
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 481/1024
Learning rate: [0.000125]
train loss=1.175401, train psnr=27.216456, train time=155.200269
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 482/1024
Learning rate: [0.000125]
train loss=1.176943, train psnr=27.381209, train time=155.450784
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 483/1024
Learning rate: [0.000125]
train loss=1.178097, train psnr=27.224029, train time=155.324922
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 484/1024
Learning rate: [0.000125]
train loss=1.178600, train psnr=27.241115, train time=155.303995
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 485/1024
Learning rate: [0.000125]
train loss=1.178226, train psnr=27.206386, train time=155.431856
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 486/1024
Learning rate: [0.000125]
train loss=1.178269, train psnr=27.217135, train time=154.735234
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 487/1024
Learning rate: [0.000125]
train loss=1.177095, train psnr=27.239008, train time=155.288036
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 488/1024
Learning rate: [0.000125]
train loss=1.176857, train psnr=27.276230, train time=154.754048
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 489/1024
Learning rate: [0.000125]
train loss=1.174617, train psnr=27.300158, train time=155.113203
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 490/1024
Learning rate: [0.000125]
train loss=1.175774, train psnr=27.255953, train time=154.905009
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 491/1024
Learning rate: [0.000125]
train loss=1.178238, train psnr=27.227903, train time=155.104814
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 492/1024
Learning rate: [0.000125]
train loss=1.176409, train psnr=27.273672, train time=155.043315
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 493/1024
Learning rate: [0.000125]
train loss=1.176180, train psnr=27.297342, train time=155.189065
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 494/1024
Learning rate: [0.000125]
train loss=1.178756, train psnr=27.141314, train time=154.871421
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 495/1024
Learning rate: [0.000125]
train loss=1.179635, train psnr=27.150201, train time=154.890823
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 496/1024
Learning rate: [0.000125]
train loss=1.174896, train psnr=27.212971, train time=155.100055
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 497/1024
Learning rate: [0.000125]
train loss=1.176442, train psnr=27.345841, train time=155.066777
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 498/1024
Learning rate: [0.000125]
train loss=1.176152, train psnr=27.291039, train time=155.199553
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 499/1024
Learning rate: [0.000125]
train loss=1.177252, train psnr=27.268005, train time=154.994307
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 500/1024
Learning rate: [0.000125]
train loss=1.176883, train psnr=27.252977, train time=155.094683
Validating with hparam 0.0
Validating with hparam 1.0
Saved checkpoint to /share/sablab/nfs02/users/aw847/models/HyperRecon/div_prior_3terms/Sep_29/archhyperunet_methoduniform_diversity_prior_rate4_lr0.001_bs32_l1+ssim_hnet128_unet32_topKNone_restrictTrue_hpNone_beta128.0/checkpoints/model.0500.h5

Epoch 501/1024
Learning rate: [0.000125]
train loss=1.176980, train psnr=27.239208, train time=154.838044
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 502/1024
Learning rate: [0.000125]
train loss=1.179675, train psnr=27.162636, train time=155.160853
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 503/1024
Learning rate: [0.000125]
train loss=1.177584, train psnr=27.169393, train time=155.137050
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 504/1024
Learning rate: [0.000125]
train loss=1.176305, train psnr=27.224088, train time=155.084097
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 505/1024
Learning rate: [0.000125]
train loss=1.179200, train psnr=27.229358, train time=154.894296
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 506/1024
Learning rate: [0.000125]
train loss=1.176470, train psnr=27.170840, train time=154.820257
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 507/1024
Learning rate: [0.000125]
train loss=1.175481, train psnr=27.209408, train time=154.837851
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 508/1024
Learning rate: [0.000125]
train loss=1.178922, train psnr=27.183680, train time=154.970104
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 509/1024
Learning rate: [0.000125]
train loss=1.177052, train psnr=27.290802, train time=155.425907
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 510/1024
Learning rate: [0.000125]
train loss=1.178612, train psnr=27.171815, train time=155.007785
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 511/1024
Learning rate: [0.000125]
train loss=1.175439, train psnr=27.293059, train time=154.952868
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 512/1024
Learning rate: [0.000125]
train loss=1.179129, train psnr=27.234937, train time=154.908561
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 513/1024
Learning rate: [6.25e-05]
train loss=1.174812, train psnr=27.344052, train time=155.051518
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 514/1024
Learning rate: [6.25e-05]
train loss=1.175049, train psnr=27.284865, train time=154.802088
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 515/1024
Learning rate: [6.25e-05]
train loss=1.174329, train psnr=27.311113, train time=155.609851
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 516/1024
Learning rate: [6.25e-05]
train loss=1.172908, train psnr=27.385497, train time=155.196807
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 517/1024
Learning rate: [6.25e-05]
train loss=1.176554, train psnr=27.202435, train time=155.020775
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 518/1024
Learning rate: [6.25e-05]
train loss=1.175306, train psnr=27.286303, train time=154.921253
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 519/1024
Learning rate: [6.25e-05]
train loss=1.176752, train psnr=27.319429, train time=154.938997
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 520/1024
Learning rate: [6.25e-05]
train loss=1.176280, train psnr=27.349849, train time=154.973887
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 521/1024
Learning rate: [6.25e-05]
train loss=1.173361, train psnr=27.379533, train time=154.956737
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 522/1024
Learning rate: [6.25e-05]
train loss=1.170823, train psnr=27.358415, train time=155.206884
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 523/1024
Learning rate: [6.25e-05]
train loss=1.175240, train psnr=27.243353, train time=155.090191
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 524/1024
Learning rate: [6.25e-05]
train loss=1.173583, train psnr=27.342555, train time=155.185337
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 525/1024
Learning rate: [6.25e-05]
train loss=1.172758, train psnr=27.398791, train time=155.156205
Validating with hparam 0.0
Validating with hparam 1.0
Saved checkpoint to /share/sablab/nfs02/users/aw847/models/HyperRecon/div_prior_3terms/Sep_29/archhyperunet_methoduniform_diversity_prior_rate4_lr0.001_bs32_l1+ssim_hnet128_unet32_topKNone_restrictTrue_hpNone_beta128.0/checkpoints/model.0525.h5

Epoch 526/1024
Learning rate: [6.25e-05]
train loss=1.176346, train psnr=27.273745, train time=155.399489
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 527/1024
Learning rate: [6.25e-05]
train loss=1.172536, train psnr=27.372426, train time=154.927944
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 528/1024
Learning rate: [6.25e-05]
train loss=1.174457, train psnr=27.367821, train time=155.218835
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 529/1024
Learning rate: [6.25e-05]
train loss=1.173361, train psnr=27.369404, train time=155.054462
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 530/1024
Learning rate: [6.25e-05]
train loss=1.176144, train psnr=27.344411, train time=155.101316
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 531/1024
Learning rate: [6.25e-05]
train loss=1.176558, train psnr=27.271163, train time=155.165602
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 532/1024
Learning rate: [6.25e-05]
train loss=1.172286, train psnr=27.380322, train time=154.988250
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 533/1024
Learning rate: [6.25e-05]
train loss=1.173635, train psnr=27.383569, train time=155.254983
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 534/1024
Learning rate: [6.25e-05]
train loss=1.171901, train psnr=27.306722, train time=155.046039
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 535/1024
Learning rate: [6.25e-05]
train loss=1.171632, train psnr=27.369230, train time=155.365572
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 536/1024
Learning rate: [6.25e-05]
train loss=1.172423, train psnr=27.363441, train time=154.927259
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 537/1024
Learning rate: [6.25e-05]
train loss=1.174917, train psnr=27.362491, train time=155.405824
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 538/1024
Learning rate: [6.25e-05]
train loss=1.171751, train psnr=27.347057, train time=154.962934
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 539/1024
Learning rate: [6.25e-05]
train loss=1.172435, train psnr=27.322336, train time=156.530360
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 540/1024
Learning rate: [6.25e-05]
train loss=1.172743, train psnr=27.264954, train time=164.663222
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 541/1024
Learning rate: [6.25e-05]
train loss=1.172889, train psnr=27.343261, train time=164.099615
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 542/1024
Learning rate: [6.25e-05]
train loss=1.174336, train psnr=27.366876, train time=164.406610
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 543/1024
Learning rate: [6.25e-05]
train loss=1.177962, train psnr=27.275299, train time=164.034557
Validating with hparam 0.0
Validating with hparam 1.0

Epoch 544/1024
Learning rate: [6.25e-05]
